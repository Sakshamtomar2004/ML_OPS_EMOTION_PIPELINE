{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b2800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/Sakshamtomar2004/ML_OPS_EMOTION_PIPELINE.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q dagshub 'mlflow>=2,<3'\n",
    "pip install tensorflow librosa tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folders = [\n",
    "    \"artifacts/data/raw\",\n",
    "    \"artifacts/data/processed\",\n",
    "    \"artifacts/data/clips_5sec\",\n",
    "    \"artifacts/features\"\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "print(\"Folder structure created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b2d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r /content/ML_OPS_EMOTION_PIPELINE/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f78283d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b2c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import argparse\n",
    "import yaml\n",
    "import hashlib\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project directory to the Python path\n",
    "project_dir = \"/content/ML_OPS_EMOTION_PIPELINE\"\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "\n",
    "from src.logger import logging\n",
    "from src.exception import MyException\n",
    "from src.data_ingestion import DataIngestion\n",
    "from src.feature_extraction import FeatureExtraction\n",
    "from src.model_training import ModelTraining\n",
    "\n",
    "\n",
    "# ---------------- Helper Functions ---------------- #\n",
    "\n",
    "def get_config_hash(config_section: dict) -> str:\n",
    "    \"\"\"\n",
    "    Create a hash for a given section of config.\n",
    "    Ensures we can detect if parameters have changed.\n",
    "    \"\"\"\n",
    "    config_str = json.dumps(config_section, sort_keys=True)\n",
    "    return hashlib.md5(config_str.encode()).hexdigest()\n",
    "\n",
    "\n",
    "def save_hash(hash_val: str, hash_file: str):\n",
    "    \"\"\"Save hash string into a file.\"\"\"\n",
    "    with open(hash_file, \"w\") as f:\n",
    "        f.write(hash_val)\n",
    "\n",
    "\n",
    "def load_hash(hash_file: str) -> str:\n",
    "    \"\"\"Load hash string from a file.\"\"\"\n",
    "    if os.path.exists(hash_file):\n",
    "        with open(hash_file, \"r\") as f:\n",
    "            return f.read().strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "# ---------------- Pipeline ---------------- #\n",
    "\n",
    "def run_pipeline(config_path, force=False):\n",
    "    \"\"\"Run the complete MLflow pipeline\"\"\"\n",
    "    try:\n",
    "        # Load config\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            import dagshub\n",
    "            dagshub.init(repo_owner='tomarsaksham2006', repo_name='ML_OPS_EMOTION_PIPELINE', mlflow=True) \n",
    "            mlflow.set_tracking_uri(\"https://dagshub.com/tomarsaksham2006/ML_OPS_EMOTION_PIPELINE.mlflow/\")\n",
    "\n",
    "        # Set MLflow experiment\n",
    "        mlflow.set_experiment(config['mlflow']['experiment_name'])\n",
    "\n",
    "        with mlflow.start_run(run_name=config['mlflow']['run_name']):\n",
    "            logging.info(\"Starting emotion detection pipeline\")\n",
    "\n",
    "            # ---------------- STEP 1: DATA INGESTION ---------------- #\n",
    "            clips_csv = os.path.join(config['data']['clips_data_dir'], \"clips_metadata.csv\")\n",
    "\n",
    "            if os.path.exists(clips_csv) and not force:\n",
    "                logging.info(f\"Step 1 Skipped: Clips metadata found at {clips_csv}\")\n",
    "                df_clips = pd.read_csv(clips_csv)\n",
    "            else:\n",
    "                logging.info(\"Step 1: Data Ingestion started...\")\n",
    "                data_ingestion = DataIngestion(config_path)\n",
    "                data_ingestion.download_data()\n",
    "                df_clips = data_ingestion.clip_5sec_segments()\n",
    "                logging.info(\"Step 1: Data Ingestion completed\")\n",
    "\n",
    "            # ---------------- STEP 2: FEATURE EXTRACTION ---------------- #\n",
    "            feat_dir = \"artifacts/features\"\n",
    "            os.makedirs(feat_dir, exist_ok=True)\n",
    "\n",
    "            chunks_pkl = os.path.join(feat_dir, \"features_chunks.pkl\")\n",
    "            hash_file = os.path.join(feat_dir, \"features_config_hash.txt\")\n",
    "\n",
    "            # Compute current config hash for feature extraction\n",
    "            current_hash = get_config_hash(config['features'])\n",
    "            saved_hash = load_hash(hash_file)\n",
    "\n",
    "            trainer = ModelTraining(config_path)  # Needed for feature loading\n",
    "\n",
    "            if os.path.exists(chunks_pkl) and saved_hash == current_hash and not force:\n",
    "                logging.info(\"Step 2 Skipped: Features already extracted and config unchanged.\")\n",
    "                features, labels_cat, labels_subcat = trainer.load_features()\n",
    "            else:\n",
    "                logging.info(\"Step 2: Feature Extraction started...\")\n",
    "                feature_extractor = FeatureExtraction(config_path)\n",
    "                features_chunks, cats_chunks, subcats_chunks = feature_extractor.extract_all_features(df_clips)\n",
    "                # Save hash of config\n",
    "                save_hash(current_hash, hash_file)\n",
    "                logging.info(\"Step 2: Feature Extraction completed\")\n",
    "\n",
    "            # ---------------- STEP 3: MODEL TRAINING ---------------- #\n",
    "            logging.info(\"Step 3: Model Training started...\")\n",
    "\n",
    "            # Load features again (ensures we have them after extraction)\n",
    "            features, labels_cat, labels_subcat = trainer.load_features()\n",
    "\n",
    "            # Prepare data\n",
    "            prepared_data = trainer.prepare_data(features, labels_cat, labels_subcat)\n",
    "\n",
    "            # Build model\n",
    "            input_shape = prepared_data['X_train'].shape[1:]\n",
    "            num_categories = prepared_data['y_train_cat'].shape[1]\n",
    "            num_subcategories = prepared_data['y_train_subcat'].shape[1]\n",
    "\n",
    "            model = trainer.build_model(input_shape, num_categories, num_subcategories)\n",
    "\n",
    "            # Train model\n",
    "            model, history = trainer.train_model(model, prepared_data)\n",
    "\n",
    "            # Save model\n",
    "            trainer.save_model(model, prepared_data)\n",
    "\n",
    "            logging.info(\"Step 3: Model Training completed\")\n",
    "            logging.info(\"Pipeline completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Pipeline failed: {e}\")\n",
    "        raise MyException(e, sys) from e\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config_path\", type=str, default=\"/content/ML_OPS_EMOTION_PIPELINE/configs/config.yaml\")\n",
    "    parser.add_argument(\"--force\", action=\"store_true\", help=\"Force recomputation even if artifacts exist\")\n",
    "\n",
    "    # Check if running in a Colab environment and parse args accordingly\n",
    "    if 'google.colab' in sys.modules:\n",
    "        args = parser.parse_args([]) # Pass an empty list to avoid parsing Colab's args\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    run_pipeline(args.config_path, args.force)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
